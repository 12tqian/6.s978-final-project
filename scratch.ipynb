{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from .env file.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_file = \"../.env\"\n",
    "\n",
    "if os.path.exists(env_file):\n",
    "    dotenv.load_dotenv(env_file, verbose=True)\n",
    "    print(\"Loaded environment variables from .env file.\")\n",
    "\n",
    "cwd = os.getcwd()\n",
    "# for some reason appending to PATH you need it to be string\n",
    "sys.path.append(str(Path(cwd).parent / \"src\"))\n",
    "hf_access_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "# from research_tools.gpu import get_gpus_available\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(i) for i in get_gpus_available()])\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "\n",
    "tensor_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "batch_size = 256\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=tensor_transform\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=tensor_transform\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "assert device == torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "class EDMVel(torch.nn.Module):\n",
    "    def __init__(self, model=None):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        self.ema = copy.deepcopy(self.model).eval().requires_grad_(False)\n",
    "        ## parameters\n",
    "        self.sigma_min = 0.002\n",
    "        self.sigma_max = 80.0\n",
    "        self.rho = 7.0\n",
    "        self.sigma_data = 0.5\n",
    "        self.P_mean = -1.2\n",
    "        self.P_std = 1.2\n",
    "        self.sigma_data = 0.5\n",
    "        self.ema_rampup_ratio = 0.05\n",
    "        self.ema_halflife_kimg = 500\n",
    "        self.drop_prob = 0.1\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_,\n",
    "        sigma_,\n",
    "        class_labels=None,\n",
    "        augment_labels=None,\n",
    "        force_fp32=False,\n",
    "        use_ema=False,\n",
    "        **model_kwargs\n",
    "    ):\n",
    "        # sigma_[sigma_>0.988] = 0.988\n",
    "        # Assume sigma_ is in [0.001, 80/81].\n",
    "        # assert (sigma_ >= 0.001).all() and (sigma_ <= 80/81).all()\n",
    "        # Rescale x and sigma.\n",
    "        x = x_ / (1 - sigma_).view(-1, 1, 1, 1)\n",
    "        sigma = sigma_ / (1 - sigma_)\n",
    "\n",
    "        # Compute posterior mean D_x\n",
    "        x = x.to(torch.float32)\n",
    "        # augment_labels = torch.zeros([x.shape[0], self.augment_dim], device = x.device)\n",
    "        sigma = sigma.to(torch.float32).reshape(-1, 1, 1, 1)\n",
    "        # class_labels = None if self.label_dim == 0 else torch.zeros([1, self.label_dim], device=x.device) if class_labels is None else class_labels.to(torch.float32).reshape(-1, self.label_dim)\n",
    "        dtype = torch.float32\n",
    "\n",
    "        c_skip = self.sigma_data**2 / (sigma**2 + self.sigma_data**2)\n",
    "        c_out = sigma * self.sigma_data / (sigma**2 + self.sigma_data**2).sqrt()\n",
    "        c_in = 1 / (self.sigma_data**2 + sigma**2).sqrt()\n",
    "        c_noise = sigma.log() / 4\n",
    "\n",
    "        if use_ema:\n",
    "            F_x = self.ema(\n",
    "                (c_in * x).to(dtype),\n",
    "                c_noise.flatten(),\n",
    "                class_labels=class_labels,\n",
    "                **model_kwargs\n",
    "            )\n",
    "        else:\n",
    "            F_x = self.model(\n",
    "                (c_in * x).to(dtype),\n",
    "                c_noise.flatten(),\n",
    "                class_labels=class_labels,\n",
    "                **model_kwargs\n",
    "            )\n",
    "        assert F_x.dtype == dtype\n",
    "        D_x = c_skip * x + c_out * F_x.to(torch.float32)\n",
    "\n",
    "        # Compute v_t\n",
    "        v_t = (x_ - D_x) / sigma_.view(-1, 1, 1, 1)\n",
    "        return v_t\n",
    "\n",
    "    def update_ema(self, step):\n",
    "        ema_halflife_nimg = self.ema_halflife_kimg * 1000\n",
    "        if self.ema_rampup_ratio is not None:\n",
    "            ema_halflife_nimg = min(\n",
    "                ema_halflife_nimg, step * batch_size * self.ema_rampup_ratio\n",
    "            )\n",
    "        ema_beta = 0.5 ** (batch_size / max(ema_halflife_nimg, 1e-8))\n",
    "        for p_ema, p_net in zip(self.ema.parameters(), self.model.parameters()):\n",
    "            p_ema.copy_(p_net.detach().lerp(p_ema, ema_beta))\n",
    "\n",
    "    def round_sigma(self, sigma):\n",
    "        return torch.as_tensor(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks_edm import SongUNet\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    unet = SongUNet(\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        num_blocks=2,\n",
    "        attn_resolutions=[0],\n",
    "        model_channels=32,\n",
    "        channel_mult=[1, 2, 2],\n",
    "        dropout=0.13,\n",
    "        img_resolution=28,\n",
    "        label_dim=10,\n",
    "        label_dropout=0.1,\n",
    "        embedding_type=\"positional\",\n",
    "        encoder_type=\"standard\",\n",
    "        decoder_type=\"standard\",\n",
    "        augment_dim=0,\n",
    "        channel_mult_noise=1,\n",
    "        resample_filter=[1, 1],\n",
    "    )\n",
    "    pytorch_total_grad_params = sum(\n",
    "        p.numel() for p in unet.parameters() if p.requires_grad\n",
    "    )\n",
    "    print(\n",
    "        f\"total number of trainable parameters in the Score Model: {pytorch_total_grad_params}\"\n",
    "    )\n",
    "    pytorch_total_params = sum(p.numel() for p in unet.parameters())\n",
    "    print(f\"total number of parameters in the Score Model: {pytorch_total_params}\")\n",
    "    return unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of trainable parameters in the Score Model: 1703041\n",
      "total number of parameters in the Score Model: 1703041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2875864/709536505.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  unet.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "model_path = Path(\"edm_model_checkpoint.pth\")\n",
    "\n",
    "unet = create_model().to(device)\n",
    "unet.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_edm = EDMVel(model=unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_time_schedule(n_steps_1, n_steps_2, split=0.5):\n",
    "    time_schedule = []\n",
    "    split = 1 - split\n",
    "    for i in range(n_steps_2):\n",
    "        time_schedule += [split * (i / n_steps_2)]\n",
    "\n",
    "    for i in range(n_steps_1 + 1):\n",
    "        time_schedule += [split + (1 - split) * (i / n_steps_1)]\n",
    "\n",
    "    time_schedule = list(reversed(time_schedule))\n",
    "    time_schedule[0] = 1 - 1e-5\n",
    "    return time_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def v_edm_sampler(\n",
    "    v_edm,\n",
    "    latents,\n",
    "    class_labels=None,\n",
    "    num_steps=18,\n",
    "    guide_w=0.0,\n",
    "    use_ema=True,\n",
    "    cfg=True,\n",
    "    drop_prob=0.0,\n",
    "    suppress_print=False,\n",
    "):\n",
    "    time_schedule = [(i / num_steps) for i in reversed(range(1, num_steps + 1))] + [0]\n",
    "    time_schedule[0] = 1 - 1e-5\n",
    "    # time_schedule = get_split_time_schedule(6, 12)\n",
    "    cnt = 0\n",
    "    if not suppress_print:\n",
    "        print(f\"Time schedule: {time_schedule}\")\n",
    "\n",
    "    if class_labels is not None:\n",
    "        class_labels = (\n",
    "            nn.functional.one_hot(class_labels, num_classes=10)\n",
    "            .type(torch.float32)\n",
    "            .to(latents.device)\n",
    "        )\n",
    "        mask = torch.bernoulli(torch.ones_like(class_labels) * (1 - drop_prob))\n",
    "        class_labels = class_labels * mask\n",
    "        if cfg:\n",
    "            class_labels = torch.cat([class_labels, torch.zeros_like(class_labels)])\n",
    "    # Main sampling loop.\n",
    "    x_next = latents.to(torch.float64).to(device) * time_schedule[0]\n",
    "\n",
    "    rand_x = random.choice([10 + i for i in range(8)])\n",
    "    rand_y = random.choice([10 + i for i in range(8)])\n",
    "\n",
    "    random_trajs = [\n",
    "        [x_next[i, 0, rand_x, rand_y].detach().cpu().item()]\n",
    "        for i in range(latents.shape[0])\n",
    "    ]\n",
    "    middle = None\n",
    "    for i in tqdm(range(len(time_schedule[:-1]))):\n",
    "        t = (\n",
    "            torch.ones((latents.shape[0] * (2 if cfg else 1),), device=device)\n",
    "            * time_schedule[i]\n",
    "        )\n",
    "        t_next = (\n",
    "            torch.ones((latents.shape[0] * (2 if cfg else 1),), device=x_next.device)\n",
    "            * time_schedule[i + 1]\n",
    "        )\n",
    "        dt = t_next[0] - t[0]\n",
    "        if cfg:\n",
    "            x_hat = x_next.repeat(2, 1, 1, 1)\n",
    "        else:\n",
    "            x_hat = x_next\n",
    "\n",
    "        vt = v_edm(x_hat, t, class_labels, use_ema=use_ema).to(torch.float64)\n",
    "        x_next = x_hat + vt * dt\n",
    "\n",
    "        # cfg guide\n",
    "        if cfg:\n",
    "            x_next_cond, x_next_uncond = x_next.chunk(2)\n",
    "            x_next = (1 + guide_w) * x_next_cond - guide_w * x_next_uncond\n",
    "        if i == (num_steps // 2):\n",
    "            middle = x_next.clone().detach()\n",
    "\n",
    "    return x_next, middle, class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch_size: int):\n",
    "\n",
    "    x_T = torch.randn([batch_size, 1, 28, 28], device=device)\n",
    "    # random from 0 to 9 inclusive\n",
    "    c = torch.randint(0, 10, (batch_size,), device=device)\n",
    "    x_gen, _, c = v_edm_sampler(\n",
    "        v_edm, x_T, class_labels=c, num_steps=18, guide_w=4.5, suppress_print=True\n",
    "    )\n",
    "    # x_gen = (x_gen / 2 + 0.5).clamp(0, 1)\n",
    "    return x_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 18/18 [00:01<00:00, 11.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception Score: 1.95 ± 0.04\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import functional as TF\n",
    "from torchvision.models import inception_v3\n",
    "from scipy.stats import entropy\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "# Function to preprocess MNIST images for Inception-v3\n",
    "def preprocess_for_inception(images):\n",
    "    # Rescale images to 299x299 and convert to RGB\n",
    "    images = torch.stack([TF.resize(img, (299, 299)) for img in images])\n",
    "    images = images.repeat(1, 3, 1, 1)  # Convert grayscale to RGB\n",
    "    return images\n",
    "\n",
    "\n",
    "# Function to compute Inception Score\n",
    "def compute_inception_score(num_samples=5000, batch_size=32, splits=10, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Computes the Inception Score for a generative model.\n",
    "    - generator: the generative model that generates MNIST images\n",
    "    - num_samples: number of samples to generate for evaluation\n",
    "    - batch_size: number of images per batch\n",
    "    - splits: number of splits for the score calculation\n",
    "    - device: 'cuda' or 'cpu'\n",
    "    \"\"\"\n",
    "    # Load the Inception-v3 model\n",
    "    inception = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "    inception.double()\n",
    "    inception.eval()\n",
    "\n",
    "    # Generate samples\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples // batch_size):\n",
    "            # Generate images\n",
    "\n",
    "            fake_images = generate_batch(batch_size)\n",
    "            fake_images = preprocess_for_inception(fake_images)\n",
    "            # should be double\n",
    "            fake_images = fake_images.to(torch.float64)\n",
    "\n",
    "            # Compute predictions\n",
    "            preds = torch.softmax(inception(fake_images), dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "\n",
    "    # Compute Inception Score\n",
    "    split_scores = []\n",
    "    for k in range(splits):\n",
    "        part = all_preds[\n",
    "            k * (num_samples // splits) : (k + 1) * (num_samples // splits)\n",
    "        ]\n",
    "        py = np.mean(part, axis=0)\n",
    "        scores = [entropy(p, py) for p in part]\n",
    "        split_scores.append(np.exp(np.mean(scores)))\n",
    "\n",
    "    return np.mean(split_scores), np.std(split_scores)\n",
    "\n",
    "\n",
    "v_edm.eval()\n",
    "\n",
    "is_mean, is_std = compute_inception_score(\n",
    "    num_samples=1000, batch_size=1000, splits=5, device=\"cuda\"\n",
    ")\n",
    "print(f\"Inception Score: {is_mean:.2f} ± {is_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99999, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0] 11\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10\n",
    "time_schedule = [(i / num_steps) for i in reversed(range(1, num_steps + 1))] + [0]\n",
    "time_schedule[0] = 1 - 1e-5\n",
    "print(time_schedule, len(time_schedule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99999, 0.9166666666666667, 0.8333333333333333, 0.75, 0.6666666666666666, 0.5833333333333334, 0.5, 0.3333333333333333, 0.16666666666666666, 0.0] 10\n"
     ]
    }
   ],
   "source": [
    "n_steps_1 = 6\n",
    "n_steps_2 = 3\n",
    "\n",
    "# split 0.5\n",
    "time_schedule = []\n",
    "for i in range(n_steps_2):\n",
    "    time_schedule += [0.5 * (i / n_steps_2)]\n",
    "\n",
    "for i in range(n_steps_1 + 1):\n",
    "    time_schedule += [0.5 + 0.5 * (i / n_steps_1)]\n",
    "\n",
    "time_schedule = list(reversed(time_schedule))\n",
    "time_schedule[0] = 1 - 1e-5\n",
    "print(time_schedule, len(time_schedule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, Compose\n",
    "from torchvision.models import inception_v3\n",
    "from scipy.linalg import sqrtm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Preprocessing for MNIST to fit Inception-v3 input\n",
    "def preprocess_mnist(images):\n",
    "    \"\"\"\n",
    "    Resize MNIST images to 299x299 and convert grayscale to RGB.\n",
    "    Args:\n",
    "        images (torch.Tensor): MNIST images (B, 1, 28, 28)\n",
    "    Returns:\n",
    "        torch.Tensor: Preprocessed images (B, 3, 299, 299)\n",
    "    \"\"\"\n",
    "    transform = Compose(\n",
    "        [\n",
    "            Resize((299, 299)),\n",
    "            Normalize(mean=[0.5], std=[0.5]),  # Normalize grayscale images\n",
    "        ]\n",
    "    )\n",
    "    images = images.repeat(1, 3, 1, 1)  # Convert grayscale to RGB\n",
    "    images = torch.stack([transform(img) for img in images])  # Apply transformations\n",
    "    return images\n",
    "\n",
    "\n",
    "# Compute the FID score\n",
    "def compute_fid(real_loader, fake_loader, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Compute FID score between real and generated images.\n",
    "    Args:\n",
    "        real_loader (DataLoader): DataLoader for real images.\n",
    "        fake_loader (DataLoader): DataLoader for generated images.\n",
    "        device (str): Device to run the computations ('cuda' or 'cpu').\n",
    "    Returns:\n",
    "        float: FID score.\n",
    "    \"\"\"\n",
    "    # Load Inception-v3 model\n",
    "    inception = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "    inception.eval()\n",
    "    inception.double()\n",
    "\n",
    "    def get_activations(data_loader):\n",
    "        activations = []\n",
    "        with torch.no_grad():\n",
    "            for batch in data_loader:\n",
    "                images, _ = batch  # Labels are not needed\n",
    "                images = preprocess_mnist(images).to(device)  # Apply preprocessing here\n",
    "                images = images.to(torch.double)\n",
    "                preds = inception(images)\n",
    "                activations.append(preds.cpu().numpy())\n",
    "        return np.concatenate(activations, axis=0)\n",
    "\n",
    "    # Compute activations for real and fake images\n",
    "    real_activations = get_activations(real_loader)\n",
    "    fake_activations = get_activations(fake_loader)\n",
    "\n",
    "    # Calculate mean and covariance of activations\n",
    "    mu_real, sigma_real = np.mean(real_activations, axis=0), np.cov(\n",
    "        real_activations, rowvar=False\n",
    "    )\n",
    "    mu_fake, sigma_fake = np.mean(fake_activations, axis=0), np.cov(\n",
    "        fake_activations, rowvar=False\n",
    "    )\n",
    "\n",
    "    # Compute FID score using the formula\n",
    "    diff = mu_real - mu_fake\n",
    "    covmean = sqrtm(sigma_real @ sigma_fake)\n",
    "    # Handle numerical issues\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    fid = diff @ diff + np.trace(sigma_real + sigma_fake - 2 * covmean)\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:01<00:00, 14.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset for real images\n",
    "real_dataset = MNIST(root=\"./data\", train=False, transform=ToTensor(), download=True)\n",
    "use_len = 1000\n",
    "\n",
    "real_dataset = torch.utils.data.Subset(real_dataset, list(range(use_len)))\n",
    "\n",
    "real_loader = DataLoader(real_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "v_edm.eval()\n",
    "# Generate a dataset of fake images\n",
    "fake_images = []\n",
    "\n",
    "fake_images = generate_batch(len(real_dataset))\n",
    "fake_images = fake_images.to(torch.float64)\n",
    "# list\n",
    "fake_images_list = []\n",
    "for i in range(len(fake_images)):\n",
    "    fake_images_list.append((fake_images[i].detach().cpu(), 0))\n",
    "\n",
    "fake_images = fake_images_list\n",
    "\n",
    "\n",
    "fake_loader = DataLoader(fake_images, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mnt/align1_drive/tcqian/unlearning_order/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID Score: 305.43\n"
     ]
    }
   ],
   "source": [
    "# Compute FID score\n",
    "fid_score = compute_fid(real_loader, fake_loader, device=\"cuda\")\n",
    "print(f\"FID Score: {fid_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = None\n",
    "for x in real_loader:\n",
    "    d = x[0][2]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 76.95it/s]\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0608598068356514..1.0691249370574951].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# plot fake_images\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "def show_images(images, nmax=64):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.imshow(make_grid(images.detach()[:nmax], nrow=8).permute(1, 2, 0))\n",
    "\n",
    "\n",
    "fake = generate_batch(1).detach().cpu()\n",
    "show_images(fake[0], nmax=32)\n",
    "# show_images(d, nmax=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0258, dtype=torch.float64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0173, dtype=torch.float64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
